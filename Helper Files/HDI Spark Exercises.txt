

	. Create Spark Cluster using Portal
	
	. Using Jupyter Notebooks
	
			Session 1: Magics
			
				%%help
				%%info
				%%sql
				%%configure
				%%logs
				
							
	
			Session 1.1: Read and Write from WASB(s)
			
			textLines = spark.sparkContext.textFile('wasb:///example/data/gutenberg/ulysses.txt')
			seqFile = spark.sparkContext.sequenceFile('wasb:///example/data/people.seq')
			parquetFile = spark.read.parquet('wasb:///example/data/people.parquet')
			jsonFile = spark.read.json('wasb:///example/data/people.json')
			csvFile = spark.read.csv('wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv', header=True, inferSchema=True)
			
			textLines.saveAsTextFile('wasb:///example/data/gutenberg/ulysses2py.txt')
			parquetRDD = parquetFile.rdd
			parquetRDD.saveAsTextFile('wasb:///example/data/peoplepy.txt')
			
			parquetFile.write.json('wasb:///example/data/people3py.json')
			csvFile.write.parquet('wasb:///example/data/people3py.parquet')
			jsonFile.write.csv('wasb:///example/data/people3py.csv')	
			seqFile.saveAsSequenceFile('wasb:///example/data/people2py.seq')
			
			
			
			
			Session 1.2: Essential Data Processing Features in Spark
			
			%%sql
			SHOW TABLES

			fruits = spark.sparkContext.textFile('wasb:///example/data/fruits.txt')
			yellowThings = spark.sparkContext.textFile('wasb:///example/data/yellowthings.txt')

			fruitsReversed = fruits.map(lambda fruit: fruit[::-1])

			fruitsAndYellowThings = fruits.union(yellowThings)

			fruitsArray = fruits.collect()
			fruitsArray

			numFruits = fruits.count()
			numFruits

			first3Fruits = fruits.take(3)
			first3Fruits

			df = spark.read.csv('wasb:///HdiSamples/HdiSamples/SensorSampleData/building/building.csv', header=True, inferSchema=True)

			df.show()

			df.printSchema()

			dfrdd = df.rdd
			dfrdd.take(3)

			df.select('BuildingID', 'Country').limit(3).show()

			df.groupBy('HVACProduct').count().select('HVACProduct', 'count').show()

			df.registerTempTable('HVAC')

			%%sql
			SELECT * FROM HVAC WHERE BuildingAge >= 10

			%%sql 
			SELECT BuildingID, Country FROM HVAC LIMIT 3
			
			
			
			
	
			Session 2: Load Data and Run Queries on an Spark Cluster
			
			from pyspark.sql import *
			from pyspark.sql.types import *
			
			# Create an RDD from sample data
			csvFile = spark.read.csv('wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv', header=True, inferSchema=True)
			csvFile.write.saveAsTable("hvac")
			
			%%sql
			SELECT buildingID, (targettemp - actualtemp) AS temp_diff, date FROM hvac WHERE date = \"6/1/13\"
						
						
						
						
			Session 3: Analyze Spark data using Power BI in HDInsight
			
			
			
			
			
			Session 4: Working with Hive tables using Spark
			
			%%sql
			SHOW TABLES
			
			%%sql -q -m sample -r 0.1 -n 500 -o query2 
			SELECT * FROM hivesampletable
			
			%%local  
			print(len(query2))
			
			hivesampletabledf = spark.table('hivesampletable')
			
			hivesampletablequerydf = spark.sql("""
				SELECT clientid, querytime, deviceplatform, querydwelltime 
				FROM hivesampletable 
				WHERE state = 'Washington' AND devicemake = 'Microsoft' AND querydwelltime > 15
				""")
				
			hivesampletablequerydf.show()
			
			%%sql -q
			CREATE TABLE IF NOT EXISTS hivesampletablecopypy ( 
                    clientid string, 
                    querytime string, 
                    market string, 
                    deviceplatform string,
                    devicemake string,
                    devicemodel string,
                    state string, 
                    country string,
                    querydwelltime double,
                    sessionid bigint,
                    sessionpagevieworder bigint )
					
			from pyspark.sql import DataFrameWriter

			dfw = DataFrameWriter(hivesampletabledf)
			dfw.insertInto('hivesampletablecopypy', overwrite=True)
			
			
			
			
			
			Session 5: Analyzing Website Logs in Spark Cluster
			
			from pyspark.sql import Row
			from pyspark.sql.types import *

			logs = sc.textFile('wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/SampleLog/909f2b.log')

			logs.take(5)

			sc.addPyFile('wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py')

			def parse_line(l):
				import iislogparser
				return iislogparser.parse_log_line(l)

			logLines = logs.map(parse_line).filter(lambda p: p is not None).cache()

			logLines.take(2)
			
			errors = logLines.filter(lambda p: p.is_error())
			numLines = logLines.count()
			numErrors = errors.count()
			print 'There are', numErrors, 'errors and', numLines, 'log entries'
			errors.map(lambda p: str(p)).saveAsTextFile('wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/SampleLog/909f2b-2.log')
			
			def avgTimeTakenByKey(rdd):
				return rdd.combineByKey(lambda line: (line.time_taken, 1),
										lambda x, line: (x[0] + line.time_taken, x[1] + 1),
										lambda x, y: (x[0] + y[0], x[1] + y[1]))\
					.map(lambda x: (x[0], float(x[1][0]) / float(x[1][1])))
				
			avgTimeTakenByKey(logLines.map(lambda p: (p.cs_uri_stem, p))).top(25, lambda x: x[1])
			
			avgTimeTakenByMinute = avgTimeTakenByKey(logLines.map(lambda p: (p.datetime.minute, p))).sortByKey()
			
			schema = StructType([StructField('Minutes', IntegerType(), True),
                     StructField('Time', FloatType(), True)])
					 
			avgTimeTakenByMinuteDF = spark.createDataFrame(avgTimeTakenByMinute, schema)
			
			avgTimeTakenByMinuteDF.registerTempTable('AverageTime')
			
			%%sql -o averagetime
			SELECT * FROM AverageTime

			%%local
			%matplotlib inline
			import matplotlib.pyplot as plt

			plt.plot(averagetime['Minutes'], averagetime['Time'], marker='o', linestyle='--')
			plt.xlabel('Time (min)')
			plt.ylabel('Average time taken for request (ms)')
			
			
			